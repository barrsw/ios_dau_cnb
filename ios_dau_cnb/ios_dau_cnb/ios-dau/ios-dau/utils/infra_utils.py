# Databricks notebook source
from sw_data_artifact_databricks.spark_data_artifact import SparkDataArtifact
from pyspark.sql import DataFrameWriter, DataFrame, Row, Column, Window
from datetime import date, timedelta, datetime
import datetime as dt
from typing import Union, List, Dict, Tuple
import pyspark.sql.functions as F
import pyspark.sql.types as T
from dateutil import relativedelta
import calendar
from functools import reduce
from pyspark.sql.functions import col

# COMMAND ----------

def generate_env_list(envs_str: str) -> list[dict]:
    """
    Generate env list (data-sources list) for SparkDataArtifact
    :param envs_str: string of envs separated by commas
    :return: env list for SparkDataArtifact
    """
    
    return [{'type': 's3a', 'name': 'sw-apps-core-data-buffer', 'prefix': f'/ios_dau_phoenix/{env.strip()}'} for env in envs_str.split(',')]

# COMMAND ----------

def generate_prod_env() -> list[dict]:
    """
    Generate env list (data-sources list) for SparkDataArtifact of production
    :return: env list for SparkDataArtifact
    """
    
    return [{'type': 's3a', 'name': 'sw-apps-core-data-buffer', 'prefix': ''}, {'type': 's3a', 'name': 'sw-apps-core-data', 'prefix': ''}]

# COMMAND ----------

def write_output(df_writer: DataFrameWriter, path: str, envs: list[dict], overwrite):
    """
    Writes DataFrame to given path with first env in envs list
    :param df_writer: DataFrameWriter which holds '.write.load("...")'
    :param path: Relative path to write to.
    :param envs: envs dict list as generated by 'generate_env_list' function.
    :param overwrite: whether to overwrite data or not.
    """
    
    absolute_path = f"{envs[0]['type']}://{envs[0]['name']}/{envs[0]['prefix']}/{path}"
    if overwrite:
        print("Writing.... overwrite on")
        df_writer.mode("overwrite").save(absolute_path)
        print(f"Writing completed to {absolute_path}")
    else:
        print("Trying to write if collection doesn't exist")
        try: 
            df_writer.save(absolute_path)
            print(f"Writing completed to {absolute_path}")
        except Exception as e:
            if "already exists" in str(e) :
                print("Data already exists; to overwrite, set 'overwrite' flag to true")
            else:
                raise e

# COMMAND ----------

def add_year_month(d: Union[datetime, str]) -> str:
    if isinstance(d, str):
        d = datetime.strptime(d, "%Y-%m-%d")
    return "/year={:02d}/month={:02d}/".format(d.year%2000, d.month)

def add_year_month_day(d: Union[datetime, str]) -> str:
    if isinstance(d, str):
        d = datetime.strptime(d, "%Y-%m-%d")
    return "/year={:02d}/month={:02d}/day={:02d}/".format(d.year%100, d.month, d.day)

def add_date(self, year_n_digits = 4):
    return self.withColumn("date", F.concat_ws("-", (F.col("year").cast("int") + (F.lit(year_n_digits) - F.length("year"))*F.lit(1000)).cast("string"), F.lpad("month", 2, '0'), F.lpad("day", 2, '0')))

def add_YMD(self, year_n_digits = 4):
    return self.withColumn("year", F.substring(F.year("date"), 4-year_n_digits + 1, year_n_digits)).withColumn("month", F.lpad(F.month(F.col("date")), 2, '0')).withColumn("day", F.lpad(F.dayofmonth(F.col("date")), 2, '0'))
DataFrame.add_date = add_date
DataFrame.add_YMD = add_YMD    

# COMMAND ----------

def get_previous_month(date: str) -> datetime:
    return datetime.strptime(date, "%Y-%m-%d") - relativedelta.relativedelta(months=1)

# COMMAND ----------

def get_last_date_in_month(d: str) -> date:
    d = datetime.strptime(d, "%Y-%m-%d")
    return date(d.year, d.month, calendar.monthrange(d.year, d.month)[1])

# COMMAND ----------

def get_days_in_month(d: str) -> int:
    d = datetime.strptime(d, "%Y-%m-%d")
    return calendar.monthrange(d.year, d.month)[1]

# COMMAND ----------

def generate_paths_for_date_range(start, end, date_format = "%Y-%m-%d", path = "", year_n_digits=4, days_back = 0): 
    ss = datetime.strptime(start, date_format)
    ee = datetime.strptime(end, date_format)
    date_list = [ee - dt.timedelta(d) for d in range((ee - ss).days + days_back + 1)]
    path_list = [f"{path}/year={str(d.year)[(4-year_n_digits):]}/month={d.month:02d}/day={d.day:02d}" for d in date_list]
    return path_list

# COMMAND ----------

def read_between_dates(start, end, path = "", days_back = 0, envs = None, date_format = "%Y-%m-%d") -> DataFrame: 
    ss = datetime.strptime(start, date_format)
    ee = datetime.strptime(end, date_format)
    date_list = [ee - dt.timedelta(d) for d in range((ee - ss).days + days_back + 1)]
    dfs = []
    for d in date_list:
        df = (SparkDataArtifact().read_dataframe(path + "/" + add_year_month_day(d), spark.read.format("parquet"), debug=True, data_sources=envs)
              .withColumn("date", F.lit(d.strftime("%Y-%m-%d")).cast("date")))
        dfs.append(df)
    return reduce(DataFrame.unionByName, dfs)

# COMMAND ----------

def path_exists(path):
    try:
        dbutils.fs.ls(path)
        return True
    except:
        return False

# COMMAND ----------

def calc_daily_executions(start_date, end_date):
    return [start_date + timedelta(days=i) for i in range((end_date-start_date).days + 1)]

def calc_weekly_executions(start_date, end_date):
    if start_date.weekday() != 6: # if start_date isn't sunday get start_date as next sunday
        idx = (start_date.weekday() + 1) % 7
        start_date = start_date + timedelta(7-idx) # Next sunday
        print("start_date is modifed to " + str(start_date))
    return [start_date + timedelta(7*d) for d in range(int((end_date-start_date).days/7)+1)]

def calc_monthly_executions(start_date, end_date):
    if start_date.day != 1: # if start_date isn't first day of month
        raise Exception("Dates in monthly mode should start on the first day of the month")
    dates = []
    while start_date <= end_date:
        dates.append(start_date)
        start_date = start_date + relativedelta.relativedelta(months=1, day=1)
    return dates

def get_dates_range(start_date, end_date, mode='daily'):
    assert mode in ["daily", "weekly", "monthly"]
    if isinstance(start_date, str):
        start_date = datetime.strptime(start_date, "%Y-%m-%d")
    if isinstance(end_date, str):
        end_date = datetime.strptime(end_date, "%Y-%m-%d")
    execution_funcs = {"daily": calc_daily_executions, "weekly": calc_weekly_executions, "monthly": calc_monthly_executions}
    return execution_funcs[mode](start_date, end_date)

# COMMAND ----------

def get_sunday_of_n_weeks_ago(d, n):
    if isinstance(d, str):
        d = datetime.strptime(d, "%Y-%m-%d").date()
    return d - timedelta((n * 7) + ((d.weekday() + 1) % 7))

def get_measure_protocol_dates(base_path, d, weeks=12, mp_start_date=date(2022, 11, 27)):
    
    if isinstance(d, str):
        d = datetime.strptime(d, "%Y-%m-%d").date()
    
    # For earlier execution date than mp_start_date set execution date to mp_start_date + weeks
    if d < mp_start_date + timedelta(weeks=weeks):
        d = mp_start_date + timedelta(weeks=weeks)

    mp_last_date = get_sunday_of_n_weeks_ago(d, 1)
    mp_first_date = mp_last_date - timedelta(weeks=weeks - 1)
    dates = get_dates_range(mp_first_date, mp_last_date, "weekly")
    return [f"{base_path}{add_year_month_day(d)}" for d in dates]

# COMMAND ----------

def read_and_union(paths: List[str], envs=None, allowMissingColumns=False) -> DataFrame:
    dfs = [SparkDataArtifact().read_dataframe(p, spark.read.format("parquet"), debug=True, data_sources=envs) for p in paths]
    res = dfs[0]
    for df in dfs[1:]:
        res = res.unionByName(df, allowMissingColumns)
    return res

# COMMAND ----------

#Make sure 'days' column exists
def calc_coverage(df: DataFrame, min_active_users: float = 10.0) -> DataFrame:
    days = len(df.select("day").distinct().collect())
    return (df.groupBy("country", "app")
            .agg((F.sum("active_users")/days).alias("mean_active_users"))
            .filter(f"mean_active_users >= {min_active_users}")
            .groupBy("country")
            .agg(F.count_distinct("app").alias("coverage")))

def calc_accuracy(df):
    return df.groupBy("country", "bucket").agg(F.avg("mape").alias("mape"), F.avg("swacc").alias("swacc"), F.avg("n_apps").alias("n_apps")).orderBy("country", "bucket")

# COMMAND ----------

def get_widget_or_default(name: str, default) -> str:
    try:
        return dbutils.widgets.get(name)
    except:
        return default

# COMMAND ----------

def read_NB_params(params_path: str, min_hits:int, remove_apple: bool, envs, min_cond_p: float = -1, include_lab: bool = True) -> DataFrame:
    return (SparkDataArtifact().read_dataframe(params_path, spark.read.format("parquet"), debug=True, data_sources=ENVS)
              .transform(lambda df: df.filter(((F.col("hits") >= min_hits) & F.col("is_lab") == False) | (F.col("is_lab") == True)) if include_lab else df.filter(F.col("hits") >= min_hits))
              .transform(lambda df: df.filter(f"cond_p >= {min_cond_p}") if min_cond_p != -1 else df)
              .transform(lambda df: df.filter("host NOT LIKE '%apple%'") if remove_apple else df))

# COMMAND ----------

def parse_model_string(models_str):
    return [{"model_path": model_path, "model_date": model_date, "model_type": model_type, 'is_main_model': is_main_model == 'True'} for (model_path, model_date, model_type, is_main_model) in [model_str.split(',') for model_str in models_str.split(';')]]